---
title: "Week 5"
title-block-banner: true
title-block-style: default
execute:
  freeze: true
  cache: true
# server: shiny
# format: 
#   revealjs:
#     # smaller: true
#     scrollable: true
#     fragments: true
#     chalkboard: true
format: html
# format: pdf
---

## Agenda:

1. Interpretation of regression coefficients
2. Categorical covariates
3. Multiple regression
    - Extension from simple linear regression
    - Other topics

## Packages we will require this week

```{r}
#| echo: true
library(tidyverse)
library(ISLR2)
library(cowplot)
library(kableExtra)
```

# Tue, Feb 7


## What is the interpretation of $\beta_0$ and $\beta_1$?

The regression model is given as follows:

$$
y_i  = \beta_0 + \beta_1 x_i + \epsilon_i
$$

where:

1. $y_i$ are the response
2. $x_i$ is the covariate
3. $\epsilon_i$ is the error (vertical black line in lecture 4 notes)
4. $\beta_0$ and $\beta_1$ are the regression coefficients
5. $i = 1, 2, \dots, n$ are the indices for the observations


Can anyone tell me the interpretation for the regression coefficients?

$\beta_0$ is the intercept and $\beta_1$ is the slope. 

Let's consider the following example using `mtcars`

```{r}
library(ggplot2)

mtcars %>% head() %>% kable()
```

Consider the following relationship

```{r}
x <- mtcars$hp
y <- mtcars$mpg

plot(x, y, pch=20, xlab="HP", ylab="MPG")

model <- lm(y~x)
summary(model)
```

For the intercept this means that:

> A "hypothetical" car with `hp=0` will have `mpg = 30.09` = $\beta_0$

It's more interesting and instructive to consider the interpretation of the slope:

Let's say we have some covariate $x_0$ then the expected value for $y(x_0)$ is given  by

$$y(x_0) = \beta_0 + \beta_1 x_0$$

What's the expected value for $x_0 + 1$?

$$
\begin{align}
y(x_0 + 1) &= \beta_0 + \beta_1 \times (x_0 + 1)\\ \\
&= \beta_0 + \beta_1 x_0 + \beta_1\\ \\
&= y(x_0) + \beta_1\\ \\
\implies \beta_1 &= y(x_0 + 1) - y(x_0)
\end{align}
$$


<br><br><br><br><br><br>
---

## Categorical covariates

Up until now, we have looked at _simple_ linear regression models where both $x$ and $y$ are quantitative. 


Let's confirm that `cyl` is indeed categorical:

```{r}
mtcars$cyl
```

Another example we have is with the iris dataset:

```{r}
iris %>% head() %>% kable()
```

Let's consider the following example:

We want to see if there is a relationship between `species` and `sepal.length`. How would we start the EDA?


```{r}
y <- iris$Sepal.Length
x <- iris$Species

boxplot(Sepal.Length ~ Species, iris)
```


Let's just run a linear regression model and see what the model output is going to look like:

```{r}
cat_model <- lm(Sepal.Length ~ Species, iris)
cat_model
```

Even if $x$ is categorical, we can still write down the regression model as follows:

$$
y_i = \beta_0 + \beta_1 x_i
$$

where $x_i \in \{ setosa, \ versicolor, \ virginica \}$. This means that we end up with, (fundamentally) three different models

1. $y_i = \beta_0 + \beta \times \mathbf{1}(x_i = \texttt{setosa})$
2. $y_i = \beta_0 + \beta \times \mathbf{1}(x_i = \texttt{versicolor})$
3. $y_i = \beta_0 + \beta \times \mathbf{1}(x_i = \texttt{virginica})$


Now, the interpretation for the coefficients are as follows:

##### Intercept

$\beta_0$ is the expected $y$ value when $x$ belongs to the base category. This is what the intercept is capturing. 


##### Slopes

$\beta_1$ with the name `Species.versicolor` represents the following

`(Intercept)` = $y(x = \texttt{setosa})$

`Species.versicolor` = $y(x = \texttt{versicolor}) - y(x = \texttt{setosa})$

`Species.virginica` = $y(x = \texttt{virginica}) - y(x = \texttt{setosa})$



### Reordering the factors

Let's say that we didn't want `setosa` to be the baseline level, and, instead, we wanted  `virginica` to be the baseline level. How would we do this?


First, we're going to reorder/relevel the categorical covariate

```{r}

summary(iris$Species) # before

iris %>% 
mutate(Species = relevel(Species, "virginica")) %>% 
head(3)

summary(iris$Species) # After
```


Once we do the releveling, we can now run the regression model:

```{R}
new_cat_model <- lm(Sepal.Length ~ Species, iris)
new_cat_model
```


<br><br><br><br><br><br>

# Thu, Feb 9

Packages for today:

```{R}
library(plotly)
```


## Multiple Regression

This is the extension of simple linear regression to multiple covariates $X = [x_1 | x_2 | \dots | x_p]$, i.e., 

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots \beta_p x_p + \epsilon
$$

In particular, the data looks like the following:

| $\mathbf y$ | $\mathbf x_1$ | $\mathbf x_2$ | $\dots$ | $\mathbf x_p$ |
|:---------|:---------|:---------|:---------|:---------|
|$y_{1}$   |$x_{1,1}$ |$x_{2,1}$ |${...}$ |$x_{p,1}$ |
|$y_{2}$   |$x_{1,2}$ |$x_{2,2}$ |${...}$ |$x_{p,2}$ |
|$y_{3}$   |$x_{1,3}$ |$x_{2,3}$ |${...}$ |$x_{p,3}$ |
|${\vdots}$ |${\vdots}$ |${\vdots}$ |$\ddots$ |$\vdots$ |
|$y_{n}$   |$x_{1,n}$ |$x_{2,n}$ |${...}$ |$x_{p,n}$ |

and, the full description of the model is as follows:

$$
y_i = \beta_0 + \beta_1 x_{1, i} + \beta_2 x_{2, i} + \dots + \beta_p x_{p, i} + \epsilon_i,
$$

Consider the `Credit` dataset

```{R}
attach(ISLR2::Credit)
df <- Credit %>% tibble() %>% rename_all(tolower)
df
```

and, we'll look at the following three columns: `income, rating, limit`.

```{R}
df3 <- df %>% select(income, rating, limit)
```

If we want to see how the credit limit is related to income and ccredit rating, we can visualize the following plot

```{R}
fig <- plot_ly(df3, x=~income, y=~rating, z=~limit)
fig %>% add_markers()
```

The regression model is as follows:

```{R}
model <- lm(limit ~ income + rating, df3)
model
```

What does the regression model look like here?

```{R}
ranges <- df3 %>%
    select(income, rating) %>%
    colnames() %>%
    map(\(x) seq(0.1 * min(df3[x]), 1.1 * max(df3[x]), length.out = 50))

b <- model$coefficients
z <- outer(
    ranges[[1]], 
    ranges[[2]], 
    Vectorize(function(x2, x3) {
        b[1] + b[2] * x2 + b[3] * x3
    })
)
fig %>%
    add_surface(x = ranges[[1]], y = ranges[[2]], z = t(z), alpha=0.3) %>% 
    add_markers()

```


<br><br><br><br><br><br>
<br><br><br><br><br><br>

## Multiple regression

## Example

